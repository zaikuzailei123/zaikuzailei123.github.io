---
layout: post
title: 矩阵分解用于推荐系统算法要解决的
date: 2020-09-29
categories: blog
tags: [技术,协同过滤,推荐算法,机器学习]
description: 2020-09-29-RSA不对称加密

html:
  embed_local_images: true
  embed_svg: false
  offline: false
  toc: undefined 
    print_background: false
export_on_save:
  html: true
---

## 1. 矩阵分解用于推荐算法要解决的问题
这是一篇转载的文章，文章[链接](https://www.cnblogs.com/pinard/p/6351319.html)  

在推荐系统中，我们常常遇到问题是这样的，我们有很多用户和物品，也有少部分物品的评分，我们希望预测目标用户对其他未评分物品进行评分，进而将评分高的物品推荐给目标用户。比如下面的用户物品评分表：

| 用户/物品|物品1 |物品2 |物品3|物品4|物品5|物品6|物品7|
|:----:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|用户1|3| |5| | |1| |
|用户2||2|||||4|
|用户3||||4||||
|用户4|||2||||1|
|用户5|1||||4||  


对于每个用户，我们希望较准确的预测用户对未评分物品的评分。对于这个问题我们有许多解决方法。本文我们关注于用矩阵分解的方法来做。如果将m个用户n个物品的评分看作一个矩阵M，我们希望通过将矩阵分解来解决这个问题。

## 2. 传统的奇异值分解SVD用于推荐
说到矩阵分解，首先想到的就是奇异值分解SVD，对这个用户物品评分 $ m * n $ 的矩阵M进行SVD分解，并通过选择部分较大的奇异值来同时进行降维，也就是说矩阵M此时分解为：  

$$ M_{m\times n}=U_{m\times k}\Sigma _{k\times k}V^{T}_{k\times n} $$  

其中k是矩阵M中较大的部分奇异值的个数， 一般会远远小于用户数和物品数。如果我们要预测第i个用户对第j个物品的评分m_{ij}，则只需要计算 $ u^{T}_{i} \Sigma v_{j} $ 即可。通过这种方法，我们可以将评分表里面所有有评分的位置得到一个评分预测。通过找到最高的若干评分对应的物品推荐给用户。  

可以看出这种方法简单直接，似乎很有吸引力。**但是SVD分解要求是稠密的，也就是说矩阵的所有位置不能有空白。有空白时M没法直接去SVD分解的，而我们协同过滤的推荐任务就是将稀疏矩阵中没有评分的空白位置填写**。所以对于一个有空白的矩阵，先通过平均值、众数等等填写上，然后再SVD，算出空白位置上新的数值再反向填回去。

尽管有上面的补全策略，我们传统的VD在推荐算法上海市比较难使用，因为用户和物品都是超级大的，这么一个大矩阵做SVD分解非常耗时。

## 3. FunkSVD算法用于推荐

FunkSVD是在传统SVD面临计算效率问题时提出来的，既然将一个矩阵做SVD分解成3个矩阵很耗时，同时还要面临稀疏的问题，那么能不能避开稀疏问题，同时只分解成两个矩阵呢？也就是说，现在我们期望矩阵M这样进行分解：  

$$ M_{m\times n}=P^{T}_{m\times k}Q_{k \times n} $$  

我们知道SVD分解已经很成熟了，但是FunkSVD如何将矩阵M分解为P和Q呢？这里采用了**回归**的思想。我们的目标时让用户的评分和用矩阵乘积得到的评分残差尽可能小，也就是说，可以用均方差作为损失函数，来寻找最终的P和Q。

对于某一个用户评分 $  m_{i,j} $ ，如果用用FunkSVD 进行矩阵分解，则对应的表示为 $ q^{T}_{j}p_{i} $，采用均方差作为损失函数，则我们期望  $ (m_{ij}-q^{T}_{j}p_{i})^{2} $ 尽可能小，如果考虑所有的物品和样本的组合，则我们期望最小化下式：  

$$ \sum_{i,j}(m_{i,j}-q^{T}_{j}p_{i})^{2} $$  

只要我们能够最小化上面的式子，并求出极值所对应的 $ p_{i}, q_{i} $ ，则我们最终可以得到矩阵P和Q，那么对于任意矩阵M任意一个空白评分位置，我们可以通过 $ q_{j}^{T}p_{i} $ 计算测评分。  


当然，在实际应用中，为了防止过拟合会加入一个L2正则化项，因此正式的FunkSVD的欧化函数库表J(p,q)是这样的：  

$$ argmin_{pi,qj}\sum_{i,j}(m_{ij}-q^{T}_{j}p_{i}) + \lambda (\left \| p_{i} \right \|_{2}^{2} + \left \| q_{j}\right \|_{2}^{2}) $$  

其中 $ \lambda $ 为正则化系数，需要调参。对于这个优化问题，一般通过梯度下降法进行优化得到结果，将上式分别对 $ p_{i},q_{i} $求导，得到：  

$$ \frac{\partial p_{i}}{\partial J} = -2(m_{ij}-q^{T}_{j}p_{i})+2 \lambda p_{i} $$  

$$ \frac{\partial q_{j}}{\partial J} = -2(m_{ij}-q^{T}_{j}p_{i})+2 \lambda q_{j} $$  

则梯度迭代时， $ p_{i},q_{j} $ 的迭代公式为：  

$$ p_{i}= p_{i} + \alpha ((m_{ij}-q^{T}_{j}p_{i})- \lambda p_{i}) $$  

$$ q_{j} = q_{j} + \alpha ((m_{ij}-q^{T}_{j}p_{i})- \lambda q_{j}) $$  


通过迭代我们最终可以得到P和Q，进而用于推荐。FunkSVD算法虽然思想很简单，但是在实际应用中效果非常好，这真是验证了大道至简。

## 4.BiasSVD 算法用于推荐

在FunkSVD算法火爆之后，出现了很多FunkSVD的改进版算法。其中BiasSVD算是改进的比较成功的一种算法。BiasSVD假设评分系统包括三部分的偏置因素：一些和用户物品无关的评分因素，用户有一些和物品无关的评分因素，称为用户偏置项。而物品也有一些和用户无关的评分因素，称为物品偏置项。这其实很好理解。比如一个垃圾山寨货评分不可能高，自带这种烂属性的物品由于这个因素会直接导致用户评分低，与用户无关。

假设评分系统平均分为μ,第i个用户的用户偏置项为bi,而第j个物品的物品偏置项为bj，则加入了偏置项以后的优化目标函数J(p,q)是这样的：  

$$ argmin_{pi,qj}\sum_{i,j}(m_{ij}-\mu -b_{i} - b_{j} - q_{j}^{T}p_{i})^{2} + \lambda (\left \| p_{i}  \right \|_{2}^{2} + \left \| q_{j}  \right \|_{2}^{2} + \left \| b_{i}  \right \|_{2}^{2} +\left \| b_{j}  \right \|_{2}^{2}) $$  

同样可以用梯度下降法求出参数。

> 我的理解是，每个物品或者每个用户天生具有一些属性会左右整个协同过滤的效果，比如有些用户是刻薄的用户，对任何物品都喜欢打低分，那么这样的用户在计算评分的时候，应该是 $ q^{T}_{j}p_{i} + 物品偏置 + 用户偏置 $ ，其中用户偏置抵消了用户一贯喜欢打低分的因素，让被打低分的情况加回来。

## 5. SVD++算法用于推荐

SVD++算法在BiasSVD算法上进一步做了增强，这里它增加考虑**用户的隐式反馈**。



## 6. 小结

FunkSVD将矩阵分解用于推荐方法推到了新的高度，在实际应用中使用也是非常广泛。当然矩阵分解方法也在不停的进步，目前张量分解和分解机方法是矩阵分解推荐方法今后的一个趋势。

　　　　对于矩阵分解用于推荐方法本身来说，它容易编程实现，实现复杂度低，预测效果也好，同时还能保持扩展性。这些都是它宝贵的优点。当然，矩阵分解方法有时候解释性还是没有基于概率的逻辑回归之类的推荐算法好，不过这也不影响它的流形程度。小的推荐系统用矩阵分解应该是一个不错的选择。大型的话，则矩阵分解比起现在的深度学习的一些方法不占优势。































































































